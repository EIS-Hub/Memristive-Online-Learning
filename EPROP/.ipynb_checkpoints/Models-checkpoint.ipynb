{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False) ## for Dark plots\n",
    "#jtplot.style(theme='grade3')  ## for Light plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook translates the Eprop algorithm from tensorflow to pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_sines_target(seq_len, n_sines=4, periods=[1000, 500, 333, 200], weights=None, phases=None, normalize=True):\n",
    "    '''\n",
    "    Generate a target signal as a weighted sum of sinusoids with random weights and phases.\n",
    "    :param n_sines: number of sinusoids to combine\n",
    "    :param periods: list of sinusoid periods\n",
    "    :param weights: weight assigned the sinusoids\n",
    "    :param phases: phases of the sinusoids\n",
    "    :return: one dimensional vector of size seq_len contained the weighted sum of sinusoids\n",
    "    '''\n",
    "    if periods is None:\n",
    "        periods = [np.random.uniform(low=100, high=1000) for i in range(n_sines)]\n",
    "    assert n_sines == len(periods)\n",
    "    sines = []\n",
    "    weights = np.random.uniform(low=0.5, high=2, size=n_sines) if weights is None else weights\n",
    "    phases = np.random.uniform(low=0., high=np.pi * 2, size=n_sines) if phases is None else phases\n",
    "    for i in range(n_sines):\n",
    "        sine = np.sin(np.linspace(0 + phases[i], np.pi * 2 * (seq_len // periods[i]) + phases[i], seq_len))\n",
    "        sines.append(sine * weights[i])\n",
    "\n",
    "    output = sum(sines)\n",
    "    if normalize:\n",
    "        output = output - output[0]\n",
    "        scale = max(np.abs(np.min(output)), np.abs(np.max(output)))\n",
    "        output = output / np.maximum(scale, 1e-6)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_derivative(v_scaled, dampening_factor):\n",
    "    '''\n",
    "    Define the pseudo derivative used to derive through spikes.\n",
    "    :param v_scaled: scaled version of the voltage being 0 at threshold and -1 at rest\n",
    "    :param dampening_factor: parameter that stabilizes learning\n",
    "    :return:\n",
    "    '''\n",
    "    return torch.maximum(1 - torch.abs(v_scaled), 0) * dampening_factor\n",
    "\n",
    "class sgt_heaviside(torch.autograd.Function):\n",
    "    dampening_factor = 20.0\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, thr):\n",
    "        ctx.save_for_backward(input,thr)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > thr] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input,thr = ctx.saved_tensors\n",
    "        dE_dz = grad_output.clone()\n",
    "        #grad = grad_input/(torch.abs(sgt_heaviside.scale*(input - thr))+1.0)**2\n",
    "        dE_dz_scaled = torch.maximum(1 - torch.abs(input), 0) * dampening_factor\n",
    "        return grad, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LightLIFStateTuple = namedtuple('LightLIFStateTuple', ('v', 'z'))\n",
    "class LightLIF(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_rec, tau=20., thr=0.615, dt=1., dtype=torch.float32, dampening_factor=0.3,\n",
    "                 stop_z_gradients=False):\n",
    "        super(LightLIF, self).__init__()\n",
    "        '''\n",
    "        A tensorflow RNN cell model to simulate Learky Integrate and Fire (LIF) neurons.\n",
    "\n",
    "        WARNING: This model might not be compatible with tensorflow framework extensions because the input and recurrent\n",
    "        weights are defined with tf.Variable at creation of the cell instead of using variable scopes.\n",
    "\n",
    "        :param n_in: number of input neurons\n",
    "        :param n_rec: number of recurrenet neurons\n",
    "        :param tau: membrane time constant\n",
    "        :param thr: threshold voltage\n",
    "        :param dt: time step\n",
    "        :param dtype: data type\n",
    "        :param dampening_factor: parameter to stabilize learning\n",
    "        :param stop_z_gradients: if true, some gradients are stopped to get an equivalence between eprop and bptt\n",
    "        '''\n",
    "\n",
    "        self.dampening_factor = dampening_factor\n",
    "        self.dt = dt\n",
    "        self.n_in = n_in\n",
    "        self.n_rec = n_rec\n",
    "        self.data_type = dtype\n",
    "        self.stop_z_gradients = stop_z_gradients\n",
    "\n",
    "        self._num_units = self.n_rec\n",
    "\n",
    "        self.tau = tau\n",
    "        self._decay = torch.exp(-dt / self.tau)\n",
    "        self.thr = thr\n",
    "\n",
    "        #with tf.variable_scope('InputWeights'):\n",
    "        self.w_in_var = torch.nn.Parameter(torch.randn(n_in, n_rec, dtype=dtype) / torch.sqrt(n_in))\n",
    "        self.w_in_val = torch.nn.identity(self.w_in_var)\n",
    "\n",
    "        #with tf.variable_scope('RecWeights'):\n",
    "        self.w_rec_var = torch.nn.Parameter(torch.randn(n_rec, n_rec, dtype=dtype) / torch.sqrt(n_rec))\n",
    "        self.recurrent_disconnect_mask = torch.diag(torch.ones(n_rec)).bool()\n",
    "        self.w_rec_val = torch.where(self.recurrent_disconnect_mask, torch.zeros_like(self.w_rec_var),\n",
    "                                  self.w_rec_var)  # Disconnect autotapse\n",
    "\n",
    "    def state_size(self):\n",
    "        return LightLIFStateTuple(v=self.n_rec, z=self.n_rec)\n",
    "\n",
    "    def output_size(self):\n",
    "        return [self.n_rec, self.n_rec]\n",
    "\n",
    "    def zero_state(self, batch_size, dtype, n_rec=None):\n",
    "        if n_rec is None: n_rec = self.n_rec\n",
    "\n",
    "        v0 = torch.zeros(size=(batch_size, n_rec), dtype=dtype)\n",
    "        z0 = torch.zeros(size=(batch_size, n_rec), dtype=dtype)\n",
    "\n",
    "        return LightLIFStateTuple(v=v0, z=z0)\n",
    "\n",
    "    def forward(self, inputs, state, scope=None, dtype=torch.float32):\n",
    "        # state in tensorflow comes from the RNN cell module. We don't have that in pytorch,\n",
    "        # so that will be replaced by a simple tuple\n",
    "        thr = self.thr\n",
    "        z = state.z\n",
    "        v = state.v\n",
    "        decay = self._decay\n",
    "\n",
    "        if self.stop_z_gradients:\n",
    "            z = z.requires_grad(False)\n",
    "\n",
    "        # update the voltage\n",
    "        i_t = torch.matmul(inputs, self.w_in_val) + torch.matmul(z, self.w_rec_val)\n",
    "        I_reset = z * self.thr * self.dt\n",
    "        new_v = decay * v + (1 - decay) * i_t - I_reset\n",
    "\n",
    "        # Spike generation\n",
    "        v_scaled = (new_v - thr) / thr\n",
    "        new_z = SpikeFunction(v_scaled, self.dampening_factor)\n",
    "        new_z = new_z * 1 / self.dt\n",
    "        new_state = LightLIFStateTuple(v=new_v, z=new_z)\n",
    "        return [new_z, new_v], new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LightALIFStateTuple = namedtuple('LightALIFState', (\n",
    "    'z',\n",
    "    'v',\n",
    "    'b'))\n",
    "\n",
    "class LightALIF(LightLIF):\n",
    "    def __init__(self, n_in, n_rec, tau=20., thr=0.03, dt=1., dtype=tf.float32, dampening_factor=0.3,\n",
    "                 tau_adaptation=200., beta=1.6, stop_z_gradients=False):\n",
    "\n",
    "        super(LightALIF, self).__init__()\n",
    "        self.tau_adaptation = tau_adaptation\n",
    "        self.beta = beta\n",
    "        self.decay_b = torch.exp(-dt / tau_adaptation)\n",
    "\n",
    "    def state_size(self):\n",
    "        return LightALIFStateTuple(v=self.n_rec, z=self.n_rec, b=self.n_rec)\n",
    "\n",
    "    def output_size(self):\n",
    "        return [self.n_rec, self.n_rec, self.n_rec]\n",
    "\n",
    "    def zero_state(self, batch_size, dtype):\n",
    "        v0 = torch.zeros(size=(batch_size, self.n_rec), dtype=dtype)\n",
    "        z0 = torch.zeros(size=(batch_size, self.n_rec), dtype=dtype)\n",
    "        b0 = torch.zeros(size=(batch_size, self.n_rec), dtype=dtype)\n",
    "        return LightALIFStateTuple(v=v0, z=z0, b=b0)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, state, scope=None, dtype=tf.float32):\n",
    "        z = state.z\n",
    "        v = state.v\n",
    "        b = state.b\n",
    "        decay = self._decay\n",
    "\n",
    "        # the eligibility traces of b see the spike of the own neuron\n",
    "        new_b = self.decay_b * b + (1. - self.decay_b) * z\n",
    "        thr = self.thr + new_b * self.beta\n",
    "        if self.stop_z_gradients:\n",
    "            z.requires_grad(False)\n",
    "\n",
    "        # update the voltage\n",
    "        i_t = torch.matmul(inputs, self.w_in_val) + torch.matmul(z, self.w_rec_val)\n",
    "        I_reset = z * self.thr * self.dt\n",
    "        new_v = decay * v + (1 - decay) * i_t - I_reset\n",
    "\n",
    "        # Spike generation\n",
    "        v_scaled = (new_v - thr) / thr\n",
    "        new_z = SpikeFunction(v_scaled, self.dampening_factor)\n",
    "        new_z = new_z * 1 / self.dt\n",
    "\n",
    "        new_state = LightALIFStateTuple(v=new_v,z=new_z, b=new_b)\n",
    "        return [new_z, new_v, new_b], new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EligALIFStateTuple = namedtuple('EligALIFStateTuple', ('s', 'z', 'z_local', 'r'))\n",
    "\n",
    "class EligALIF(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_rec, tau=20., thr=0.03, dt=1., dtype=torch.float32, dampening_factor=0.3,\n",
    "                 tau_adaptation=200., beta=1.6,\n",
    "                 stop_z_gradients=False, n_refractory=1):\n",
    "        super(EligALIF, self).__init__()\n",
    "\n",
    "        if tau_adaptation is None: raise ValueError(\"alpha parameter for adaptive bias must be set\")\n",
    "        if beta is None: raise ValueError(\"beta parameter for adaptive bias must be set\")\n",
    "\n",
    "        self.n_refractory = n_refractory\n",
    "        self.tau_adaptation = tau_adaptation\n",
    "        self.beta = beta\n",
    "        self.decay_b = torch.exp(-dt / tau_adaptation)\n",
    "\n",
    "        if np.isscalar(tau): tau = torch.ones(n_rec, dtype=dtype) * torch.mean(tau)\n",
    "        if np.isscalar(thr): thr = torch.ones(n_rec, dtype=dtype) * torch.mean(thr)\n",
    "\n",
    "        tau = tau.type(dtype=dtype)\n",
    "        dt = dt.type(dtype=dtype)\n",
    "\n",
    "        self.dampening_factor = dampening_factor\n",
    "        self.stop_z_gradients = stop_z_gradients\n",
    "        self.dt = dt\n",
    "        self.n_in = n_in\n",
    "        self.n_rec = n_rec\n",
    "        self.data_type = dtype\n",
    "\n",
    "        self._num_units = self.n_rec\n",
    "\n",
    "        self.tau = tau\n",
    "        self._decay = torch.exp(-dt / tau)\n",
    "        self.thr = thr\n",
    "\n",
    "        #with tf.variable_scope('InputWeights'):\n",
    "        self.w_in_var = torch.nn.Parameter(torch.randn(n_in, n_rec) / torch.sqrt(n_in), dtype=dtype)\n",
    "        self.w_in_val = torch.nn.identity(self.w_in_var)\n",
    "\n",
    "        #with tf.variable_scope('RecWeights'):\n",
    "        self.w_rec_var = torch.nn.Parameter(torch.randn(n_rec, n_rec) / torch.sqrt(n_rec), dtype=dtype)\n",
    "        self.recurrent_disconnect_mask = torch.diag(torch.ones(n_rec)).bool()\n",
    "        self.w_rec_val = torch.where(self.recurrent_disconnect_mask, torch.zeros_like(self.w_rec_var),\n",
    "                                  self.w_rec_var)  # Disconnect self-connection\n",
    "\n",
    "        self.variable_list = [self.w_in_var, self.w_rec_var]\n",
    "        self.built = True\n",
    "\n",
    "    def state_size(self):\n",
    "        return EligALIFStateTuple(s=[self.n_rec, 2], \n",
    "                                  z=self.n_rec, r=self.n_rec, z_local=self.n_rec)\n",
    "\n",
    "    def output_size(self):\n",
    "        return [self.n_rec, [self.n_rec, 2]]\n",
    "\n",
    "    def zero_state(self, batch_size, dtype, n_rec=None):\n",
    "        if n_rec is None: n_rec = self.n_rec\n",
    "\n",
    "        s0 = torch.zeros(size=(batch_size, n_rec, 2), dtype=dtype)\n",
    "        z0 = torch.zeros(size=(batch_size, n_rec), dtype=dtype)\n",
    "        z_local0 = torch.zeros(size=(batch_size, n_rec), dtype=dtype)\n",
    "        r0 = torch.zeros(size=(batch_size, n_rec), dtype=dtype)\n",
    "\n",
    "        return EligALIFStateTuple(s=s0, z=z0, r=r0, z_local=z_local0)\n",
    "\n",
    "    def compute_z(self, v, b):\n",
    "        adaptive_thr = self.thr + b * self.beta\n",
    "        v_scaled = (v - adaptive_thr) / self.thr\n",
    "        z = SpikeFunction(v_scaled, self.dampening_factor)\n",
    "        z = z * 1 / self.dt\n",
    "        return z\n",
    "\n",
    "    def compute_v_relative_to_threshold_values(self,hidden_states):\n",
    "        v = hidden_states[..., 0]\n",
    "        b = hidden_states[..., 1]\n",
    "\n",
    "        adaptive_thr = self.thr + b * self.beta\n",
    "        v_scaled = (v - adaptive_thr) / self.thr\n",
    "        return v_scaled\n",
    "\n",
    "    def forward(self, inputs, state, scope=None, dtype=torch.float32, stop_gradient=None):\n",
    "\n",
    "        decay = self._decay\n",
    "        z = state.z\n",
    "        z_local = state.z_local\n",
    "        s = state.s\n",
    "        r = state.r\n",
    "        v, b = s[..., 0], s[..., 1]\n",
    "\n",
    "        # This stop_gradient allows computing e-prop with auto-diff.\n",
    "        #\n",
    "        # needed for correct auto-diff computation of gradient for threshold adaptation\n",
    "        # stop_gradient: forward pass unchanged, gradient is blocked in the backward pass\n",
    "        use_stop_gradient = stop_gradient if stop_gradient is not None else self.stop_z_gradients\n",
    "        if use_stop_gradient:\n",
    "            z.requires_grad(False)\n",
    "\n",
    "        new_b = self.decay_b * b + z_local # threshold update does not have to depend on the stopped-gradient-z, it's local\n",
    "\n",
    "        i_t = tf.matmul(inputs, self.w_in_val) + tf.matmul(z, self.w_rec_val) # gradients are blocked in spike transmission\n",
    "        I_reset = z * self.thr * self.dt\n",
    "        new_v = decay * v + i_t - I_reset\n",
    "\n",
    "        # Spike generation\n",
    "        is_refractory = r > 0\n",
    "        zeros_like_spikes = torch.zeros_like(z)\n",
    "        #################\n",
    "        # look at tf.where\n",
    "        #################\n",
    "        new_z = torch.where(is_refractory, zeros_like_spikes, self.compute_z(new_v, new_b))\n",
    "        new_z_local = torch.where(is_refractory, zeros_like_spikes, self.compute_z(new_v, new_b))\n",
    "        new_r = r + self.n_refractory * new_z - 1\n",
    "        new_r = torch.clamp(new_r, 0., float(self.n_refractory))\n",
    "        new_r.requires_grad(False)\n",
    "        new_s = torch.stack((new_v, new_b), dim=-1)\n",
    "\n",
    "        new_state = EligALIFStateTuple(s=new_s, z=new_z, r=new_r, z_local=new_z_local)\n",
    "        return [new_z, new_s], new_state\n",
    "\n",
    "    \n",
    "    def compute_eligibility_traces(self, v_scaled, z_pre, z_post, is_rec):\n",
    "\n",
    "        n_neurons = tf.shape(z_post)[2]\n",
    "        rho = self.decay_b\n",
    "        beta = self.beta\n",
    "        alpha = self._decay\n",
    "        n_ref = self.n_refractory\n",
    "\n",
    "        # everything should be time major\n",
    "        z_pre = z_pre.permute(1, 0, 2)\n",
    "        v_scaled = v_scaled.permute(1, 0, 2)\n",
    "        z_post = z_post.permute(1, 0, 2)\n",
    "\n",
    "        psi_no_ref = self.dampening_factor / self.thr * torch.maximum(0., 1. - torch.abs(v_scaled))\n",
    "\n",
    "        update_refractory = lambda refractory_count, z_post:\\\n",
    "            torch.where(z_post > 0,torch.ones_like(refractory_count) * (n_ref - 1),torch.maximum(0, refractory_count - 1))\n",
    "\n",
    "        refractory_count_init = torch.zeros_like(z_post[0], dtype=torch.int32)\n",
    "        #refractory_count = tf.scan(update_refractory, z_post[:-1], initializer=refractory_count_init)\n",
    "        refractory_count = []\n",
    "        refractory_count = update_refractory( refractory_count_init, z_post[:-1])\n",
    "        for i in range( 1,z_post.size(0)-1 ):\n",
    "            refractory_count.append( update_refractory( refractory_count_init, z_post[:-1]) )\n",
    "        refractory_count = torch.stack(refractory_count, dim=0)\n",
    "        refractory_count = torch.cat([[refractory_count_init], refractory_count], dim=0)\n",
    "\n",
    "        is_refractory = refractory_count > 0\n",
    "        psi = torch.where(is_refractory, tf.zeros_like(psi_no_ref), psi_no_ref)\n",
    "\n",
    "        update_epsilon_v = lambda epsilon_v, z_pre: alpha[None, None, :] * epsilon_v + z_pre[:, :, None]\n",
    "        epsilon_v_zero = torch.ones((1, 1, n_neurons)) * z_pre[0][:, :, None]\n",
    "        #epsilon_v = tf.scan(update_epsilon_v, z_pre[1:], initializer=epsilon_v_zero, )\n",
    "        epsilon_v = []\n",
    "        epsilon_past = update_epsilon_v( epsilon_past, z_pre[1] )\n",
    "        epsilon_v.append( epsilon_past )\n",
    "        for i in range( 1,z_pre[1:].size() ):\n",
    "            epsilon_past = update_epsilon_v( epsilon_past, z_pre[i] )\n",
    "            epsilon_v.append( epsilon_past )\n",
    "        epsilon_v = torch.stack( epsilon_v, dim=0 )\n",
    "        epsilon_v = tf.concat([[epsilon_v_zero], epsilon_v], dim=0)\n",
    "\n",
    "        update_epsilon_a = lambda epsilon_a, elems:\\\n",
    "                (rho - beta * elems['psi'][:, None, :]) * epsilon_a + elems['psi'][:, None, :] * elems['epsi']\n",
    "\n",
    "        ####################################################################################\n",
    "        epsilon_a_zero = tf.zeros_like(epsilon_v[0])\n",
    "        epsilon_a = tf.scan(fn=update_epsilon_a,\n",
    "                            elems={'psi': psi[:-1], 'epsi': epsilon_v[:-1], 'previous_epsi':shift_by_one_time_step(epsilon_v[:-1])},\n",
    "                            initializer=epsilon_a_zero)\n",
    "        ####################################################################################\n",
    "\n",
    "        epsilon_a = tf.concat([[epsilon_a_zero], epsilon_a], axis=0)\n",
    "\n",
    "        e_trace = psi[:, :, None, :] * (epsilon_v - beta * epsilon_a)\n",
    "\n",
    "        # everything should be time major\n",
    "        e_trace = e_trace.permute( 1, 0, 2, 3 )\n",
    "        epsilon_v = epsilon_v.permute( 1, 0, 2, 3 )\n",
    "        epsilon_a = epsilon_a.permute( 1, 0, 2, 3 )\n",
    "        psi = psi.permute( 1, 0, 2 )\n",
    "\n",
    "        if is_rec:\n",
    "            identity_diag = torch.eye(n_neurons)[None, None, :, :]\n",
    "            e_trace -= identity_diag * e_trace\n",
    "            epsilon_v -= identity_diag * epsilon_v\n",
    "            epsilon_a -= identity_diag * epsilon_a\n",
    "\n",
    "        return e_trace, epsilon_v, epsilon_a, psi\n",
    "\n",
    "    def compute_loss_gradient(self, learning_signal, z_pre, z_post, v_post, b_post,\n",
    "                              decay_out=None,zero_on_diagonal=None):\n",
    "        thr_post = self.thr + self.beta * b_post\n",
    "        v_scaled = (v_post - thr_post) / self.thr\n",
    "\n",
    "        e_trace, epsilon_v, epsilon_a, _ = self.compute_eligibility_traces(v_scaled, z_pre, z_post, zero_on_diagonal)\n",
    "\n",
    "        if decay_out is not None:\n",
    "            e_trace_time_major = e_trace.permute(1, 0, 2, 3)\n",
    "            filtered_e_zero = torch.zeros_like(e_trace_time_major[0])\n",
    "            ####################################################################################\n",
    "            filtering = lambda filtered_e, e: filtered_e * decay_out + e * (1 - decay_out)\n",
    "            filtered_e = tf.scan(filtering, e_trace_time_major, initializer=filtered_e_zero)\n",
    "            ####################################################################################\n",
    "            filtered_e = (filtered_e.permute( 1, 0, 2, 3 )\n",
    "            e_trace = filtered_e\n",
    "\n",
    "        gradient = torch.einsum('btj,btij->ij', learning_signal, e_trace)\n",
    "        return gradient, e_trace, epsilon_v, epsilon_a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
